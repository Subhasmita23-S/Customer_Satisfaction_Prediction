---
title: "Final Project"
subtitle: 'MATH 40028/50028: Statistical Learning'
date: "October 22, 2024"
output:
  html_document:
    df_print: paged
  pdf_document: default
fontfamily: mathpazo
fontsize: 11pt
header-includes: \linespread{1.05}
urlcolor: blue
---

__ACADEMIC INTEGRITY: Every student should complete the project by their own. A project report having high degree of similarity with work by any other student, or with any other document (e.g., found online) is considered plagiarism, and will not be accepted. The minimal consequence is that the student will receive the project score of 0, and the best possible overall course grade will be D. Additional consequences are described at http://www.kent.edu/policyreg/administrative-policy-regarding-student-cheating-and-plagiarism and will be strictly enforced.__

## Instruction

__Goal:__ The goal of the final project is to apply the statistical learning methods discussed in this course to perform predictive analysis of real-life data. You will need to identify prediction problem(s), carry out necessary exploratory data analysis, perform predictive analysis using statistical learning methods, assess the performance, and communicate the results in a report. 

__Report:__ Use this Rmd file as a template. Edit the file by adding your project title in the YAML, and including necessary information in the four sections: (1) Introduction, (2) Statistical learning strategies and methods, (3) Predictive analysis and results, and (4) Conclusion. 

__Submission:__ Please submit your project report as a PDF file (8-10 pages, flexible) to Canvas by __11:59 p.m. on December 8, 2024__. The PDF file should be generated by “knitting” the Rmd file. You may choose to first generate an HTML file (by changing the output format in the YAML to `output: html_document`) and then convert it to PDF. Word documents, however, cannot be used as an intermediate file (and of course, the submitted file). __20 points will be deducted if the submitted files are in wrong format.__ 

__Grade:__ The project will be graded based on your ability to (1) recognize and define prediction problems, (2) identify potentially useful statistical learning methods, (3) perform the predictive analysis and assess the performance, (4) document the analysis procedure (with R code) and clearly present the results, and (5) draw valid conclusions supported by the analysis.

__Datasets:__ You may consider (but are not restricted) to use the following packages/datasets. 

* [`ISLR2`](https://cran.rstudio.com/web/packages/ISLR2/ISLR2.pdf): datasets used in the _Introduction to Statistical Learning_ textbook
* [`dslabs`](https://cran.r-project.org/web/packages/dslabs/dslabs.pdf)
* [UCI Machine Learning Repository](https://archive.ics.uci.edu/)

\pagebreak

## Introduction [15 points]

* Describe the dataset. What is the dataset about? 
* If possible, comment on the target population, sampling strategies, potential bias, etc. 
* Identify and define prediction problem(s).
* Discuss how to split the data into training and test sets among other plans for the use of data.

## Statistical learning strategies and methods [35 points]

* Perform exploratory data analysis using the training set.
* Describe the statistical learning approaches and other strategies for feature engineering (transformation, selection, etc.).
* Based on the conditions assumed by the statistical learning methods, discuss their applicability to the prediction problem. 

## Predictive analysis and results [35 points]

* Apply and document the statistical learning procedure for the predictive analysis.
* Estimate the performance of the statistical learning approaches on test data, using resampling methods or other measures.
* Evaluate the performance on the test data.
* Discuss the results.

## Conclusion [15 points]

* Discuss the scope and generalizability of the predictive analysis. 
* Discuss potential limitations and possibilities for improvement.

#### Loading necessary libraries

```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(glmnet)
library(randomForest)
library(caret)
library(corrplot)
library(reshape2)
library(tibble)
```

#### Dataset Loading

```{r}
# loading the dataset
customer_data <- read.csv("C:\\Users\\subha\\Downloads\\customer_feedback_satisfaction.csv")
customer_data
```

```{r}
# displaying the structure of dataset
str(customer_data)
```
__Structure of Dataset:__ The above output above describes the structure of a data frame in R. It provides information about the dataset's dimensions, variable names, data types, and a preview of the first few values for each variable.


__Description of dataset:__ 

Kaggle link: https://www.kaggle.com/datasets/jahnavipaliwal/customer-feedback-and-satisfaction/data

The dataset I have chosen is from kaggle source which is a online data resource. The dataset is about customer feedback and satisfaction, with 38,444 entries and 11 columns capturing various aspects of customer demographics, purchasing behavior, and feedback. Here is a summary of the key columns:

CustomerID: Unique identifier for each customer (numeric).

Age: Age of the customer (integer).

Gender: Gender of the customer (categorical: "Male", "Female").

Country: Country of residence (categorical, e.g., "UK", "USA").

Income: Annual income of the customer in local currency (integer).

ProductQuality: A rating of product quality on a scale (possibly 1–10, integer).

ServiceQuality: A rating of service quality on a scale (possibly 1–10, integer).

PurchaseFrequency: Frequency of purchases made by the customer (integer).

FeedbackScore: Feedback classification (categorical, e.g., "Low", "Medium").

LoyaltyLevel: Customer loyalty tier (categorical, e.g., "Bronze", "Silver", "Gold").

SatisfactionScore: Overall satisfaction score (floating-point, potentially percentage or a rating out of 100).

__Target Population:__
The dataset seems to concentrate on consumers who offer product or service reviews. Most likely, this target group consists of people who:

Have bought things (as shown by "PurchaseFrequency"). Feedback was either requested or freely given. Span various nations, demographics, income brackets, and genders.

__Goal:__ Predicting the SatisfactionScore

__Prediction Problem:__
Regression Problem: Predicting the customer's satisfaction score based on demographic, behavioral, and feedback variables. 
The goal is to comprehend the factors that influence customer happiness and predict satisfaction levels precisely so that companies can take preventative action to raise them.
Features such as Age, Gender, Country, Income, ProdcutQuality, ServiceQuality, PurchaseFrequency, FeedbackScore, LoyaltyLevel are all included in predicting the SatisfactionScore. 
Understanding what factors drive satisfaction can help businesses improve products, services, and customer engagement strategies.

__Target Variable:__ SatisfactionScore (continuous value).

#### Feature Engineering

Here, I have performed some basic feature engineering steps such as Converting the categorical variable to factors, creating dummy variables according to our problem definition.

__1. Converting Categorical Variables to factors:__
To improve data quality for analysis the categorical variables 'Gender', 'Country', 'FeedbackScore', 'LoyaltyLevel' are all converted to factors ensuring they have their levels of factor.

Gender having 2 levels of factors, Country having 5 levels of factors, FeedbackScore having 3 levels of factors and LoyaltyLevel having 3 levels of factors.

```{r}
# Converting categorical variables to factors
customer_data$Gender <- factor(customer_data$Gender, levels = c("Female", "Male"))
customer_data$Country <- factor(customer_data$Country)
customer_data$FeedbackScore <- factor(customer_data$FeedbackScore, levels = c("Low", "Medium", "High"))
customer_data$LoyaltyLevel <- factor(customer_data$LoyaltyLevel, levels = c("Bronze", "Silver", "Gold"))

# Checking the levels of factor variables
levels(customer_data$Gender)
levels(customer_data$Country)
levels(customer_data$FeedbackScore)
levels(customer_data$LoyaltyLevel)
```


__2. Checking for missing values:__
Missing values are checked for each and every column using 'is.na' for the entire dataset. na.omit() filters the dataset, keeping only the rows that do not have any NA values across any columns.

```{r}
# Checking for NA values
colSums(is.na(customer_data[c("Age", "Income", "ProductQuality", "ServiceQuality", "PurchaseFrequency", "FeedbackScore", "Country", "LoyaltyLevel", "Gender")]))

# omitting the NA values, if any in the data
customer_data <- na.omit(customer_data)
```

__3. One-hot encoding (Creating dummy vars):__
Here, in this step I have created the dummy variables for the factor variables Gender, FeedbackScore, Country, and LoyaltyLevel. Dummy variables encode categorical features as numerical columns (0/1), making them compatible with these algorithms. 

__Problem with Categorical data__
Categorical data refers to variables that contain label values (categories) rather than numerical values. For example, in the cust_data dataset:

Gender may contain values like "Male" and "Female."

Country may contain values like "USA", "France", "Canada", "UK", "Germany".

FeedbackScore may contain values like "Low," "Medium," and "High."

Such data cannot be directly used by machine learning algorithms since the models can't perform mathematical operations on string values like "Male" or "Female."

__Why dummy vars?__
To address this, categorical variables need to be encoded into a numerical format that the machine learning model can understand. 
Technique one-hot encoding is used which transforms each level of a categorical variable into a separate binary (0/1) column, also known as dummy variables.

Example: 
Before Encoding: Country has 5 categories, Canada, France, Germany, UK, and USA
After Encoding: 5 new variables are created: CountryCanada, CountryFrance, CountryGermany, CountryUK, and CountryUSA.

By converting categorical data into dummy variables, it is ensured that it is numerically represented, enabling mathematical interpretation by algorithms. 
When categorical data is directly encoded as integers, such as "Male" as 1 and "Female" as 2, the algorithm may be misled into believing that "Female" (2) is "greater" than "Male" (1), which is untrue. This is avoided by one-hot encoding, which treats every category as a distinct feature.
One-hot encoding, often known as dummy variables, uses distinct binary columns to represent each category level. Accurate categorical data interpretation and computation are made possible by this transformation for the machine learning model.

```{r}
# Creating dummy variables for all categorical features
dummy_vars <- dummyVars("~ FeedbackScore + Country + LoyaltyLevel + Gender", data = customer_data)

# Generate the dummy variables dataset
dummies <- data.frame(predict(dummy_vars, newdata = customer_data))

# Combining the dummy variables with the original dataset (if needed)
customer_data <- cbind(customer_data, dummies)

# Removing original categorical columns if no longer needed
customer_data <- customer_data[, !(names(customer_data) %in% c("FeedbackScore", "Country", "LoyaltyLevel", "Gender"))]
names(customer_data) <- gsub("\\.", "", names(customer_data))
```


__4. Converting 'dbl' to 'int':__
While one-hot encoding process the dummy vars are created as 'dbl(double)' so here, I converted all the variable from 'dbl' to 'int(integer)' for efficiency and consistency while modeling.

```{r}
# Converting all dummy variables from 'dbl' to 'int'
customer_data <- customer_data %>%
  mutate(across(starts_with("FeedbackScore"), as.integer),
         across(starts_with("Country"), as.integer),
         across(starts_with("LoyaltyLevel"), as.integer),
         across(starts_with("Gender"), as.integer))
```

__5. Dataset Structure and Summary:__
Viewing and checking the dataset if all the new created variables are in 'interger' format and viewing the statistical summary of each column of the updated dataset.


```{r}
#viewing the dataset
customer_data

# Viewing the structure of the updated dataset
str(customer_data)

# Verifying the final dataset
summary(customer_data)
```


__Here, we can see that after the new variables creation there are 20 variables and 38444 rows. And also the new created dummy variables are converted to specific 0's and 1's according to the customer data.__


#### Split the data into training and test sets among other plans for the use of data

Splitting of data is done as below by dividing the customer_data into training and testing datasets for further analysis. Below is the code for the same:

80% of customer_data into train_data.

20% of customer_data into test_data.

```{r}
set.seed(123) 

# Spliting the data into training (80%) and test (20%) sets
train_index <- sample(1:nrow(customer_data), 0.7 * nrow(customer_data))

train_data <- customer_data[train_index, ]
test_data <- customer_data[-train_index, ]
```


#### Performing Exploratory Data Analysis

__1. Distribution of SatisfactionScore:__
The plot below is heavily skewed to the right, indicating that a large proportion of customers have a Satisfaction Score close to the maximum value of 100. Very few customers have lower satisfaction scores (closer to 0). The sharp spike at the Satisfaction Score of 100, indicating that the majority of customers are highly satisfied. This suggests that most customers are very happy with the service, as 100 likely represents the best satisfaction level. The distribution indicates overall positive customer sentiment, with most customers giving high scores.

```{r}
ggplot(train_data, aes(x = SatisfactionScore)) +
  geom_histogram(bins = 30, fill = "chartreuse1", color = "black") +
  labs(title = "Distribution of Satisfaction Score", x = "Satisfaction Score", y = "Count")
```


__2. SatisfactionScore by LoyaltyLevel:__
The plot below represents the SatisfactionScore based on the LoyaltyLevel(Bronze, Silver, Gold). The thick black line inside each box represents the median Satisfaction Score for that Loyalty Level. The dots below the whiskers are outliers, representing customers with much lower Satisfaction Scores compared to the majority. 

The lower bound of the whiskers reaches closer to 0 for all Loyalty Levels, suggesting that there are some very dissatisfied customers in each group. The Loyalty Level (Bronze, Silver, or Gold) does not seem to significantly impact the overall Satisfaction Score distribution, as the median and IQR are very similar across all levels. This indicates that customer satisfaction is consistently high regardless of loyalty status.
Despite the high satisfaction, there are notable outliers with lower Satisfaction Scores. This suggests that a small subset of customers may be dissatisfied, even in higher loyalty tiers like Gold.

```{r}
ggplot(train_data, aes(x = as.factor(LoyaltyLevelSilver + 2 * LoyaltyLevelGold), y = SatisfactionScore)) +
  geom_boxplot(fill = "dodgerblue2", color = "black") +
  labs(title = "Satisfaction Score by Loyalty Level", x = "Loyalty Level (0: Bronze, 1: Silver, 2: Gold)", y = "Satisfaction Score") +
  theme_minimal()
```


__3. SatisfactionScore by Gender:__
The below plot represents the distribution of SatisfactionScore by Gender. The dots below the whiskers represent outliers, indicating a small number of customers with significantly lower Satisfaction Scores. Both genders have outliers with very low Satisfaction Scores (close to 0), but the number of outliers is slightly higher for males (Gender = 1). The whiskers (lines extending from the boxes) show the range of Satisfaction Scores, excluding outliers. Both genders have whiskers that extend down to about 25, indicating a similar lower range of satisfaction. Most customers, regardless of gender, have high Satisfaction Scores (median near 100, with most scores above 75).A small subset of customers from both genders report very low Satisfaction Scores (outliers), indicating dissatisfaction. 

```{r}
ggplot(train_data, aes(x = as.factor(GenderMale), y = SatisfactionScore)) +
  geom_boxplot(fill = "turquoise1", color = "black") +
  labs(title = "Satisfaction Score by Gender", x = "Gender (0: Female, 1: Male)", y = "Satisfaction Score") +
  theme_minimal()
```

__4. SatisfactionScore by Country:__
The below plot represents the box plot of SatisfactionScore by Country. The median satisfaction scores for all four countries appear to be very close to each other and are around 90. This suggests that the central tendency of satisfaction scores is similar across the countries. The whiskers and outliers extend lower than the boxes, showing some individuals have significantly lower satisfaction scores in all countries. The overall patterns (medians, IQRs, and whiskers) are highly consistent across all four countries. This suggests no major differences in satisfaction score distributions between France, Germany, the UK, and the USA. There are numerous outliers (dots) with very low satisfaction scores in all countries, indicating a few participants from each country reported much lower satisfaction than the rest.

```{r}
# Creating a boxplot to visualize Satisfaction Score by country
ggplot(train_data, aes(x = as.factor(CountryFrance + 2 * CountryGermany + 3 * CountryUK + 4 * CountryUSA), y = SatisfactionScore)) +  
  geom_boxplot(fill = "mediumorchid1", color = "black") +
  labs(title = "Satisfaction Score by Country", 
       x = "Country (1: France, 2: Germany, 3: UK, 4: USA)", 
       y = "Satisfaction Score") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotates x-axis labels for better readability
```

__5. Counts of FeedbackScore:__
The below bar plot depicts the counts of FeedbackScore which are categorized into 3: FeedbackScoreLow, FeedbackScoreMedium, FeedbackScoreHigh. All three feedback levels have similar counts. This suggests that the dataset is well-balanced across these categories. Each category has a count close to 7,500 individuals. The balance in the data means there is no significant overrepresentation or underrepresentation of any feedback level. This is useful for prediction problems because balanced data ensures better model performance and fairness across categories.

```{r}
# Summarize feedback counts from columns starting with "FeedbackScore"
feedback_counts <- train_data %>%
  select(starts_with("FeedbackScore")) %>%    # Select columns that start with "FeedbackScore"
  summarise_all(sum) %>%                      # Compute the sum for each selected column
  pivot_longer(cols = everything(), names_to = "Feedback", values_to = "Count")

ggplot(feedback_counts, aes(x = Feedback, y = Count, fill = Feedback)) +
  geom_bar(stat = "identity") +
  labs(title = "Counts of Feedback Levels",
       x = "Feedback Level",
       y = "Count") +
  theme_minimal() +
  scale_fill_manual(values = c("FeedbackScoreLow" = "slategray",     # Assign custom colors to feedback levels
                               "FeedbackScoreMedium" = "slateblue", 
                               "FeedbackScoreHigh" = "sienna"))
```

__6. Distribution of Country- Pie Chart:__
The graph illustrates that about equal representation from each of the five nations is included in the dataset. Germany(19.8%) and the UK(19.8%) have a little smaller proportion than France(20.2%) and the USA(20.2%), although the differences are not very great. In order to facilitate fair cross-country comparisons in later analysis, this balance makes sure that no one nation dominates the dataset.

```{r}
# Calculating the distribution of Country
country_distribution <- train_data %>%
  summarise(across(starts_with("Country"), sum)) %>%
  pivot_longer(cols = everything(), names_to = "Country", values_to = "Count")

# Renaming countries for cleaner labels
country_distribution$Country <- gsub("Country", "", country_distribution$Country)

# Ploting the pie chart
pie(
  country_distribution$Count,
  labels = paste0(country_distribution$Country, " (", 
                  round(100 * country_distribution$Count / sum(country_distribution$Count), 1), "%)"),
  col = c("palevioletred", "tomato", "slateblue", "rosybrown1", "plum"),
  main = "Distribution of Country - Pie Chart",
  border = "white"
)
```

__7. Distribution of FeedbackScore- Pie Chart:__
The chart shows that roughly one-third of customers gave low feedback, one-third gave medium feedback, and one-third gave high feedback. This balance makes it easier to study the factors that influence these feedback categories and build models to predict customer satisfaction (SatisfactionScore) using this variable as a predictor.

```{r}
# Summarizing the counts for each FeedbackScore level
feedback_distribution <- train_data %>%
  summarise(across(starts_with("FeedbackScore"), sum)) %>%
  pivot_longer(cols = everything(), names_to = "FeedbackScore", values_to = "Count")

# Cleaning up FeedbackScore names for the labels
feedback_distribution$FeedbackScore <- gsub("FeedbackScore", "", feedback_distribution$FeedbackScore)

# Plot the pie chart
pie(
  feedback_distribution$Count,
  labels = paste0(feedback_distribution$FeedbackScore, " (", 
                  round(100 * feedback_distribution$Count / sum(feedback_distribution$Count), 1), "%)"),
  col = c("cadetblue1", "darkolivegreen3", "cyan3"),
  main = "Distribution of FeedbackScore - Pie Chart",
  border = "white"
)
```

__8. Distribution of Gender- Pie Chart:__
The pie chart shows that the dataset has nearly the same number of male and female customers, with females being slightly more (50.4% vs. 49.6%). This balance ensures that gender-related analyses or predictions won't be biased toward one gender. 

```{r}
# Summarizing the counts for each Gender level
gender_distribution <- train_data %>%
  summarise(across(starts_with("Gender"), sum)) %>%
  pivot_longer(cols = everything(), names_to = "Gender", values_to = "Count")

# Cleaning up Gender names for the labels
gender_distribution$Gender <- gsub("Gender", "", gender_distribution$Gender)

# Plot the pie chart
pie(
  gender_distribution$Count,
  labels = paste0(gender_distribution$Gender, " (", 
                  round(100 * gender_distribution$Count / sum(gender_distribution$Count), 1), "%)"),
  col = c("maroon3", "peachpuff2"),
  main = "Distribution of Gender - Pie Chart",
  border = "white"
)
```

#### Feature Selection

__Correlation Matrix:__
Our dataset contains 20 feature variables and 1 target variable, it is meaningless to take all the 20 variables for further modeling process as some of the variables are not related to the target variable and some variables have very less amount of correlation with the target variable which does not effect the prediction.

Along the diagonal, there are perfect positive correlations (1.0), meaning that variables correlate exactly with one another. As a result of their mutual exclusion, GenderMale and GenderFemale have a substantial negative correlation (-1.0). 
SatisfactionScore and FeedbackScoreHigh (~0.56) show moderately positive correlations, whereas PurchaseFrequency and SatisfactionScore (~0.25) show a less degree of association, indicating that frequent buyers are generally happier. 
The dummy variables show slight negative or zero correlations with each other, indicating their exclusivity. A moderate negative correlation (~-0.5) exists between FeedbackScoreHigh and FeedbackScoreLow, logically reflecting their inverse relationship. 

```{r}
corr_matrix <- cor(train_data)     # Computing the correlation matrix for the training dataset
melted_corr <- melt(corr_matrix)   # Converting the correlation matrix into a long format for visualization

# Creating a heatmap to visualize the correlation matrix
ggplot(data = melted_corr, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "palevioletred2", high = "purple1", mid = "white", midpoint = 0) +
  theme_minimal() +
  labs(title = "Correlation Matrix", x = "", y = "") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  geom_text(aes(label = round(value, 2)), color = "black", size = 3)
```

__Principal Component Analysis:__
Next, we are using Principal Component Analysis (PCA) which is one of the feature selection method.

```{r}
# Load the factoextra library
library(factoextra)

# Selecting numeric columns (exclude the target variable SatisfactionScore)
numeric_data <- train_data[, sapply(train_data, is.numeric)]

# Checking for missing values in the numeric data
sum(is.na(numeric_data))

# Performing PCA on the numeric data
pca_result <- prcomp(numeric_data, center = TRUE, scale. = TRUE)

# Visualizing variables in the principal component space (biplot)
fviz_pca_var(pca_result, col.var = "cos2",
              gradient.cols = c("yellow", "orange", "red"), repel = TRUE)

# Print summary of PCA results
summary(pca_result)
```
Above PCA Bi-plot says, the length of the arrow represents how much the variable contributes. Longer arrows indicate stronger contributions to the component. Variables that are close to each other are correlated. Variables on opposite sides are negatively correlated. Variables at right angles are uncorrelated.

__Variables with High Contribution:__
SatisfactionScore, ServiceQuality, ProductQuality features are strongly represented by the first two dimensions. Their arrows are long and have high cos2 values. GenderMale or GenderFemale can either be considered while considering for modeling. If both of them are included then collinearity can be introduced in regression models.

Below are the features that are selected for further modeling. Here, I have created a dataset 'selected_train_data' that is used to store all selected features.

```{r}
selected_train_data <- train_data[, c("SatisfactionScore", "ProductQuality", "ServiceQuality", "GenderMale")]
# Checking for missing values
sum(is.na(selected_train_data))
```

__Selected Train Data PCA Bi-plot:__
Below is the PCA Bi-plot for the selected_train_data which are considered for further modeling.

```{r}
# Selecting numeric columns (exclude the target variable SatisfactionScore)
selected_train_data <- selected_train_data[, sapply(selected_train_data, is.numeric)]

# Checking for missing values in the numeric data
sum(is.na(selected_train_data))

# Performing PCA on the numeric data
pca_result <- prcomp(selected_train_data, center = TRUE, scale. = TRUE)

# Visualizing variables in the principal component space (biplot)
fviz_pca_var(pca_result, col.var = "cos2",
              gradient.cols = c("yellow", "orange", "red"), repel = TRUE)
```

#### Modeling

Here, for modeling techniques I have used 3 modeling techniques based on the Regression problme:

__1. Linear Regression Modeling:__ Linear regression works well when satisfaction depends linearly on features like ProductQuality, ServiceQuality, and other numeric predictors. It can provide insights into how changes in these factors influence satisfaction. It assumes a linear relationship between the predictors (independent variables) and the target variable (SatisfactionScore).

__2. Random Forest Regression:__ Random forest excels when satisfaction depends on non-linear interactions among features, such as how ServiceQuality might have a different impact based on ProductQuality. It builds multiple trees (bagging) and aggregates their predictions to make a final prediction. It captures complex relationships by splitting data into decision rules at different levels.

__3. Lasso Regression:__ To the linear regression cost function, Lasso regression (Least Absolute Shrinkage and Selection Operator) applies an L1-norm penalty. This penalty essentially performs feature selection by reducing the coefficients of less significant features to precisely zero.

Here, I have used the Cross-Validation as a resampling technique to evaluate and improve the performance of statistical models, including linear regression, random forest regression, and lasso regression.

__Cross-Validation:__ Using Cross-validation with the above modeling techniques ensures that the model doesn't overfit the training data, leading to high accuracy on the training set but poor performance on unseen data. gives a robust estimate of performance metrics (e.g., RMSE, MAE, R²) by using multiple validation sets instead of relying on a single train-test split.

#### Modeling Techniques using Cross-Validation

__1. Linear Regression Modeling Technique__

```{r}
library(caret)

# Defining train control for cross-validation
train_control <- trainControl(method = "cv",  # using Cross-validation
                               number = 10,  # defining the number of folds
                               savePredictions = "final", # saving predictions for each fold
                               verboseIter = TRUE)  # displaying training progress
```

```{r}
# Fitting the linear regression model
lm_model <- train(SatisfactionScore ~ .,  # Selected predictors
                  data = selected_train_data,  # Replacing with your training dataset
                  method = "lm",  # Linear regression
                  trControl = train_control)

print(lm_model)

# View the model summary
summary(lm_model)
```

__Linear Regression Model Interpretation:__

__1. Model Performance:__
The linear regression model depicts the R-squared value is 0.6046, meaning approximately 60.46% of the variability in the satisfaction scores can be explained by the predictors in the model, and RMSE having 10.60 which suggests room for improvement. Lower RMSE values generally indicate better model performance.

__2. Significant Predictors in the Model:__
Among all the predictors in the model ProductQuality and ServiceQuality are strong and statistically significant predictors of satisfaction scores by their p-values being < 0.05. Considering GenderMale, however, is not a significant factor in the prediction considering the p-value being > 0.05.

__3. Residual Analysis of the Model:__
Considering the Residual Standard Error, residuals range from Min (-53.424) to Max (37.145), indicating some outliers exist where predictions deviate substantially from actual values. 

__4. Cross-Validation Results:__
Considering the summary of resampling results: RMS-10.60, R-squared-0.6046, MAE-8.44. These metrics indicate consistent model performance across all folds. The RMSE and R-squared in cross-validation align well with the values obtained when fitting the final model, demonstrating that the model generalizes reasonably well to unseen data.


__2. Random Forest Modeling Technique__

```{r}
library(randomForest)

# Fitting the Random Forest model
rf_model <- train(SatisfactionScore ~ .,
                  data = selected_train_data,
                  method = "rf",  # Random Forest
                  trControl = train_control,
                  tuneGrid = expand.grid(mtry = c(2, 3)),  # mtry- hyperparameters tuned
                  ntree = 200)  # Number of tuning steps

# View the best tuning parameters
print(rf_model)

summary(rf_model)
```

__Random Forest Regression Model Interpretation:__

__1. Model Performance:__
Random Forest reported the RMSE value as mtry = 2: RMSE = 9.7225; mtry = 3: RMSE = 9.7261. The model with mtry = 2 yielded the smallest RMSE, indicating better predictive performance, R-squared value being mtry = 2: R2=0.6678; mtry = 3: R2 =0.6675. Both values indicate that approximately 66.8% of the variation in the Satisfaction Score is explained by the predictors. The lower RMSE (9.7225) and higher R2 for mtry = 2 suggest that this model configuration provides the best fit among the tested options.

__2. Significant Predictors in the Model:__
The model used mtry=2, considering only 2 main variables for its evaluation. Mainly, the ProductQuality and ServiceQuality. Hyperparameter tuning 'mtry' represents the number of predictors randomly selected at each split in the trees. Based on typical Random Forest outputs, predictors like ProductQuality and ServiceQuality are likely the most significant for predicting SatisfactionScore. GenderMale, likely to have low importance score.

__3. Residual Analysis of Model:__
The RMSE of 9.7225 indicates that, on average, the model's predictions deviate from the actual scores by approximately 9.72 points.

__4. Cross-Validation results:__
The Random Forest model was evaluated using 10-fold cross-validation, ensuring that the performance metrics are robust and not over fitted to the training data. Based on the RMSE, the best-performing model is the one with mtry = 2. This model is then refit on the entire training set for final predictions.

The Random Forest model with mtry = 2 and 200 trees provides the best performance for predicting SatisfactionScore in this dataset. However, for interpretability, variable importance should be analyzed further to understand the contribution of each predictor.


__3. Lasso Regression Modeling Technique__

```{r}
# Lasso Regression (alpha = 1 for Lasso)
lasso_model <- train(SatisfactionScore ~ ., 
                     data = selected_train_data, 
                     method = "glmnet",           
                     tuneGrid = expand.grid(alpha = 1, 
                                            lambda = seq(0.001, 0.1, length = 10)),
                     trControl = train_control)

# Print and summarize results
print(lasso_model)
summary(lasso_model)
```

__Lasso Regression Model Interpretation:__

__1. Model Performance:__
Lasso regression considering the regression parameters as: Alpha=1, lambda=0.001 and 0.1, optimal hyperparameters: lambda=0.045 (Selected based on the lowest RMSE during cross-validation). Lasso regression reported minimum RMSE value 10.60 (lambda=0.045), R-squared 0.6048 indicating that about 60.5% of the variance in the Satisfaction Score is explained by the predictors. Mean Absolute Error = 8.4369, suggesting that the average absolute deviation from actual Satisfaction Scores is about 8.4 points.

__2. Residual Analysis of Model:__
The RMSE of 10.60808 suggests that the residuals (errors) have an average deviation of ~10.6 points from the actual values.

__3. Cross-Validation results:__
The Lasso regression model was evaluated using 10-fold validation, λ=0.045: This value balances model fit and regularization, minimizing RMSE while penalizing complex models. The Lasso model performs similarly to the linear regression model, with an RMSE of 10.60808 and R-square 60.5%.


__Preparing Test Data__

```{r}
selected_test_data <- train_data[, c("SatisfactionScore", "ProductQuality", "ServiceQuality", "GenderMale")]
# Checking for missing values
sum(is.na(selected_test_data))
```

#### Model Evaluation and Performance on Test Data

While considering the RMSE(Root Mean Squared Error) for the regression models, the lower RMSE value indicates the better model performance, as they reflect smaller differences between predicted and actual values.

```{r}
# Linear regression predictions and evaluation
lm_predictions <- predict(lm_model, selected_test_data)
lm_rmse <- sqrt(mean((lm_predictions - selected_test_data$SatisfactionScore)^2))
cat("Linear Regression RMSE on Test Data:", lm_rmse, "\n")

# Random Forest regression predictions and evaluation
rf_predictions <- predict(rf_model, selected_test_data)
rf_rmse <- sqrt(mean((rf_predictions - selected_test_data$SatisfactionScore)^2))
cat("Random Forest Regression RMSE on Test Data:", rf_rmse, "\n")

# Lasso regression predictions and evaluation
lasso_predictions <- predict(lasso_model, selected_test_data)
lasso_rmse <- sqrt(mean((lasso_predictions - selected_test_data$SatisfactionScore)^2))
cat("Lasso Regression RMSE on Test Data:", lasso_rmse, "\n")
```

__1. Linear Regression RMSE interpretation:__
An RMSE of 10.61 was obtained for the linear regression model. This outcome is good for a baseline model. A linear relationship between the predictors and the target variable (SatisfactionScore) is assumed by linear regression. The model's predictions are about 10.61 points off from the actual SatisfactionScore values in the test set. This RMSE value is reasonable for a linear regression model, but it also indicates room for improvement.

__2. Random Forest Regression RMSE interpretation:__
With an RMSE of 9.66, the Random Forest model outperforms the Lasso and Linear Regression models. This suggests that when it comes to SatisfactionScore prediction, Random Forest does better. The model's ability to manage non-linearities and feature interactions better than linear models may be the reason for the lower RMSE, which indicates that it is more appropriate for the dataset's complexity. 

__3. Lasso Regression RMSE interpretation:__
The Lasso regression model's RMSE of 10.61 is extremely comparable to that of the Linear Regression model. It performs similarly to Linear Regression in terms of RMSE when compared to Random Forest (9.66) and Linear Regression (10.61), indicating that while it is not as accurate as Random Forest, it performs similarly to Linear Regression. Although this model tends to reduce less significant predictors to zero, it usually performs well when there are many predictors.

#### Discussion on Results

__Best Performing Model:__ Random Forest Regression with an RMSE of 9.66.
In this instance, Random Forest provides the lowest RMSE and performs better than both Lasso and Linear regression models. It can capture intricate interactions between predictors, particularly when there are non-linear relationships between characteristics and the target variable. Because of this, Random Forest is a more reliable option for datasets with complex feature relationships.

The RMSE values of Lasso Regression and Linear Regression are similar, indicating that the dataset might not require feature selection (Lasso's strength) or the simplicity of the linear model. These models are useful for efficiency and interpretability, however Random Forest performs better in terms of prediction.

Given that the objective is to predict SatisfactionScore and that Random Forest performed the best according to the RMSE metric, Random Forest regression is advised for this dataset. The most accurate predictions should come from this model, particularly when complex interactions and non-linear correlations between predictors are included.


#### Scope and generalizability of the predictive analysis
* The SatisfactionScore is predicted using three primary predictors: Product Quality, Service Quality, and Gender. This predictive study offers important insights into how these attributes affect customer satisfaction. When addressing the extent and possible constraints of this study, there are a number of things to take into account:

 + Customer satisfaction is usually strongly predicted by the quality of the product or service. Businesses can better understand how the quality of their products and services affects customer satisfaction ratings by utilizing a model that incorporates these elements.

+ In this instance, gender is there in the dataset, but it may not be as significant as it could be, particularly if there aren't any clear gender-based patterns in satisfaction. Depending on how well it correlates with the target variable, gender may have a small impact on the model or may even be eliminated completely during regularization (for example, using Lasso).

* The RMSE-based prediction accuracy is the main focus of the analysis. Other metrics, such as model interpretability, R-squared, and MAE, however, might offer a more thorough assessment of the models' performance.

* A predictive model's ability to generalize refers to its potential to generate precise predictions on data that has not yet been observed, outside of the particular training and test datasets that were utilized. The predictive models' generalizability for this issue is determined by a number of factors:

+ Data Size: With 26,910 samples, the dataset is sizable and adequate for testing and training the majority of models. In general, models with larger datasets are more stable and less likely to overfit.

+ Data Quality: The generalizability of a dataset is influenced by its quality, including its measurement of features, the existence of missing values, and any potential biases. The model might not generalize effectively to new consumers if the data is noisy or not reflective of the larger customer base.

* With a comparatively low RMSE and strong cross-validation generalization, the predictive models—Random Forest in particular—perform admirably. However, the quality of the characteristics, the existence of significant external influences, and the data's representativeness of future trends all affect how generalizable the model is.


#### Potential limitations

1. Overfitting with Complex Models: 
If not properly adjusted, Random Forest and other tree-based models are prone to overfitting, even if they are capable of capturing extremely intricate relationships. This implies that while the model may perform well on training data, it may not generalize to new data.

2. Limited Predictors (Features):
Product quality, service quality, and gender are the only three predictors that are currently used in the models. Even while these are crucial elements, they might not account for every factor that affects consumer happiness. Other important elements that might have a big impact on SatisfactionScore are Brand Loyalty, Price Sensitivity, Location, and Past Purchase Behavior.


#### Possibilities for Improvement
* By reducing the model's complexity, regularization approaches (such as Lasso Regression) can aid in lowering overfitting. As was previously said, cross-validation can assist in fine-tuning models and preventing overfitting.

* The models can capture a more comprehensive picture of what drives customer happiness by include more pertinent features (such as consumer demographics or purchase history).

* Although SatisfactionScore can be predicted with the use of the existing predictive analysis, there are a few places where it might be strengthened to increase its accuracy and dependability. Important enhancements include of:

Enlarging the feature set to include more essential elements.
Utilizing more sophisticated models or enhancing the complexity handling of existing models.
Utilizing increasingly varied data sources and meticulous preprocessing to guarantee high-quality data.
Enhancing model interpretability and assessing models using a wider range of measures.
To make sure the model works effectively in practical situations, test its generalizability using outside data.

































